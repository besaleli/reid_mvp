{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.engine.results import Results\n",
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Load YOLOv11 model for tracking people\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "source = \"data/video_1.mp4\"\n",
    "\n",
    "results = model(source, classes=[0], iou=0.4, conf=0.5)\n",
    "\n",
    "# for memory\n",
    "del model\n",
    "\n",
    "# create df\n",
    "frames = pd.DataFrame({\"result\": results})\n",
    "frames['n_people_detected'] = frames['result'].map(lambda i: len(i.boxes))\n",
    "frames['frame_id'] = range(len(results))\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(frames, x='frame_id', y='n_people_detected', title=\"# People Detected By Frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "signal = frames['n_people_detected'].values\n",
    "model = \"l2\"\n",
    "algo = rpt.Pelt(model=model).fit(frames['n_people_detected'].values)\n",
    "breakpoints = algo.predict(pen=6)\n",
    "\n",
    "is_irregular_detection = []\n",
    "for i in range(len(breakpoints)):\n",
    "    start = 0 if i == 0 else breakpoints[i-1]\n",
    "    end = breakpoints[i]\n",
    "    dff = frames.iloc[start:end]\n",
    "    mode = dff['n_people_detected'].mode().to_list()[0]\n",
    "    print(start, end, mode)\n",
    "    is_irregular_detection += [i != mode for i in dff['n_people_detected']]\n",
    "\n",
    "frames['is_irregular_detection'] = is_irregular_detection\n",
    "\n",
    "rpt.display(signal, breakpoints, figsize=(20, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dced99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import normalize\n",
    "import torchreid\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"RUNNING ON {device}\")\n",
    "\n",
    "# Load OSNet model\n",
    "osnet_model = torchreid.models.build_model(\n",
    "    name='osnet_x1_0',\n",
    "    num_classes=1000,\n",
    "    loss='softmax',\n",
    "    pretrained=True\n",
    "    ).to(device).eval()\n",
    "\n",
    "# Transform for OSNet input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "osnet = []\n",
    "\n",
    "# preprocess for osnet\n",
    "for i, row in frames.iterrows():\n",
    "    result: Results = row['result']\n",
    "    confidences = result.boxes.conf.cpu().tolist()\n",
    "    for conf, box in zip(confidences, result.boxes.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        crop_bgr = result.orig_img[y1:y2, x1:x2]\n",
    "        if crop_bgr.size == 0:\n",
    "            continue\n",
    "        \n",
    "        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        osnet.append(\n",
    "            {\n",
    "                \"detection_id\": uuid.uuid4(),\n",
    "                \"input\": transform(crop_rgb),\n",
    "                \"frame_id\": row[\"frame_id\"],\n",
    "                \"conf\": float(conf),\n",
    "                \"x1\": x1,\n",
    "                \"y1\": y1,\n",
    "                \"x2\": x2,\n",
    "                \"y2\": y2\n",
    "                }\n",
    "            )\n",
    "\n",
    "detections = pd.DataFrame(osnet)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "embs = []\n",
    "for i in tqdm(list(range(0, len(osnet), batch_size))):\n",
    "    batch = torch.stack(detections.iloc[i:i+batch_size]['input'].to_list()).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = osnet_model(batch)\n",
    "        print(embeddings.size())\n",
    "    \n",
    "    embs.append(embeddings)\n",
    "\n",
    "detections['osnet_embedding'] = [i for i in normalize(torch.cat(embs), dim=1).cpu().numpy()]\n",
    "del detections['input']\n",
    "\n",
    "# for memory\n",
    "del osnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097860a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "# pca for coords\n",
    "detections['x_pca'], detections['y_pca'] = zip(*PCA(n_components=2).fit_transform(detections['osnet_embedding'].to_list()))\n",
    "detections['pca_50'] = [i for i in PCA(n_components=50).fit_transform(detections['osnet_embedding'].to_list())]\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "    d.x_pca,\n",
    "    d.y_pca,\n",
    "    f.is_irregular_detection\n",
    "from detections d\n",
    "left join frames f on d.frame_id=f.frame_id\n",
    "\"\"\"\n",
    "fig = px.scatter(conn.sql(query).df(), x='x_pca', y='y_pca', color='is_irregular_detection')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# mark any frames below 5th percentile confidence\n",
    "detections['conf_under_p5'] = detections['conf'] < np.quantile(detections['conf'].to_numpy(), 0.05)\n",
    "\n",
    "fig = px.histogram(detections, x='conf', title=\"Confidence Histogram\")\n",
    "fig.add_vline(x=np.quantile(detections['conf'].to_numpy(), 0.05))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3309ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "query = \"\"\"\n",
    "select\n",
    "    d.frame_id,\n",
    "    d.detection_id,\n",
    "    d.osnet_embedding,\n",
    "    f.n_people_detected\n",
    "from detections d\n",
    "left join frames f on f.frame_id = d.frame_id\n",
    "where\n",
    "    not (d.conf_under_p5 or f.is_irregular_detection)\n",
    "\"\"\"\n",
    "\n",
    "clusters = conn.query(query).df()\n",
    "\n",
    "pca_50 = PCA(n_components=50).fit_transform(clusters['osnet_embedding'].to_list())\n",
    "clusters['cluster_id_raw'] = KMeans(n_clusters=2).fit_predict(pca_50)\n",
    "\n",
    "clusters['pca_x'], clusters['pca_y'] = zip(*PCA(n_components=2).fit_transform(clusters['osnet_embedding'].to_list()))\n",
    "\n",
    "fig = px.scatter(\n",
    "    clusters,\n",
    "    x='pca_x',\n",
    "    y='pca_y',\n",
    "    color='cluster_id_raw',\n",
    "    hover_data=[\n",
    "        'cluster_id_raw',\n",
    "        'pca_x',\n",
    "        'pca_y',\n",
    "        'frame_id',\n",
    "        'n_people_detected'\n",
    "    ]\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09df411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(':memory:')\n",
    "\n",
    "query = \"\"\"\n",
    "with cpf as (\n",
    "    select\n",
    "        frame_id,\n",
    "        count(distinct cluster_id_raw) as n_distinct_clusters_per_frame\n",
    "    from clusters\n",
    "    group by 1\n",
    "    order by 1\n",
    "),\n",
    "\n",
    "bad_frames as (\n",
    "    select\n",
    "        clusters.frame_id\n",
    "    from clusters\n",
    "    left join cpf on clusters.frame_id = cpf.frame_id\n",
    "    where\n",
    "        n_people_detected > 1\n",
    "        and n_people_detected > n_distinct_clusters_per_frame\n",
    "    qualify count(*) over (partition by clusters.frame_id) > 1\n",
    "),\n",
    "\n",
    "cleaned as (\n",
    "    select\n",
    "        *,\n",
    "        frame_id in (select * from bad_frames) as is_bad_frame,\n",
    "        case\n",
    "            when frame_id in (select * from bad_frames) then -1\n",
    "            else cluster_id_raw\n",
    "        end as cluster_id\n",
    "    from clusters\n",
    ")\n",
    "\n",
    "select\n",
    "    cleaned.*,\n",
    "    detections.x1,\n",
    "    detections.y1,\n",
    "    detections.x2,\n",
    "    detections.y2\n",
    "from cleaned\n",
    "left join detections on cleaned.detection_id=detections.detection_id\n",
    "where not cleaned.is_bad_frame\n",
    "\"\"\"\n",
    "\n",
    "clean = conn.sql(query).df()\n",
    "\n",
    "fig = px.scatter(\n",
    "    clean,\n",
    "    x='pca_x',\n",
    "    y='pca_y',\n",
    "    color='cluster_id',\n",
    "    hover_data=[\"pca_x\", \"pca_y\", \"cluster_id\", \"frame_id\", \"is_bad_frame\"]\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(source)\n",
    "video_frames = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    video_frames.append(frame)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "for i, frame in enumerate(video_frames):\n",
    "    # pull up detections\n",
    "    boxes = clean[clean['frame_id'] == i]\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "    \n",
    "    for j, row in boxes.iterrows():\n",
    "        x1, y1, x2, y2 = row['x1'], row['y1'], row['x2'], row['y2']\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f'ID {row['cluster_id']}',\n",
    "            (x1, y1-10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "            )\n",
    "\n",
    "for frame in video_frames:\n",
    "    cv2.imshow(\"YOLO Detections\", frame)\n",
    "    if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a9639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
